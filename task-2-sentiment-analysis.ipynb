{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2821319,"sourceType":"datasetVersion","datasetId":1725235}],"dockerImageVersionId":30748,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-13T06:23:45.758327Z","iopub.execute_input":"2024-08-13T06:23:45.758642Z","iopub.status.idle":"2024-08-13T06:23:46.730558Z","shell.execute_reply.started":"2024-08-13T06:23:45.758615Z","shell.execute_reply":"2024-08-13T06:23:46.729516Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/go-emotions-google-emotions-dataset/go_emotions_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/go-emotions-google-emotions-dataset/go_emotions_dataset.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:46.732581Z","iopub.execute_input":"2024-08-13T06:23:46.732958Z","iopub.status.idle":"2024-08-13T06:23:48.134148Z","shell.execute_reply.started":"2024-08-13T06:23:46.732934Z","shell.execute_reply":"2024-08-13T06:23:48.133173Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:48.138412Z","iopub.execute_input":"2024-08-13T06:23:48.138747Z","iopub.status.idle":"2024-08-13T06:23:48.173757Z","shell.execute_reply.started":"2024-08-13T06:23:48.138713Z","shell.execute_reply":"2024-08-13T06:23:48.172728Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"        id                                               text  \\\n0  eew5j0j                                    That game hurt.   \n1  eemcysk   >sexuality shouldn’t be a grouping category I...   \n2  ed2mah1     You do right, if you don't care then fuck 'em!   \n3  eeibobj                                 Man I love reddit.   \n4  eda6yn6  [NAME] was nowhere near them, he was by the Fa...   \n\n   example_very_unclear  admiration  amusement  anger  annoyance  approval  \\\n0                 False           0          0      0          0         0   \n1                  True           0          0      0          0         0   \n2                 False           0          0      0          0         0   \n3                 False           0          0      0          0         0   \n4                 False           0          0      0          0         0   \n\n   caring  confusion  ...  love  nervousness  optimism  pride  realization  \\\n0       0          0  ...     0            0         0      0            0   \n1       0          0  ...     0            0         0      0            0   \n2       0          0  ...     0            0         0      0            0   \n3       0          0  ...     1            0         0      0            0   \n4       0          0  ...     0            0         0      0            0   \n\n   relief  remorse  sadness  surprise  neutral  \n0       0        0        1         0        0  \n1       0        0        0         0        0  \n2       0        0        0         0        1  \n3       0        0        0         0        0  \n4       0        0        0         0        1  \n\n[5 rows x 31 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>example_very_unclear</th>\n      <th>admiration</th>\n      <th>amusement</th>\n      <th>anger</th>\n      <th>annoyance</th>\n      <th>approval</th>\n      <th>caring</th>\n      <th>confusion</th>\n      <th>...</th>\n      <th>love</th>\n      <th>nervousness</th>\n      <th>optimism</th>\n      <th>pride</th>\n      <th>realization</th>\n      <th>relief</th>\n      <th>remorse</th>\n      <th>sadness</th>\n      <th>surprise</th>\n      <th>neutral</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>eew5j0j</td>\n      <td>That game hurt.</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>eemcysk</td>\n      <td>&gt;sexuality shouldn’t be a grouping category I...</td>\n      <td>True</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ed2mah1</td>\n      <td>You do right, if you don't care then fuck 'em!</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>eeibobj</td>\n      <td>Man I love reddit.</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>eda6yn6</td>\n      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n      <td>False</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 31 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:48.177381Z","iopub.execute_input":"2024-08-13T06:23:48.177724Z","iopub.status.idle":"2024-08-13T06:23:48.189034Z","shell.execute_reply.started":"2024-08-13T06:23:48.177692Z","shell.execute_reply":"2024-08-13T06:23:48.188002Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Index(['id', 'text', 'example_very_unclear', 'admiration', 'amusement',\n       'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity',\n       'desire', 'disappointment', 'disapproval', 'disgust', 'embarrassment',\n       'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love',\n       'nervousness', 'optimism', 'pride', 'realization', 'relief', 'remorse',\n       'sadness', 'surprise', 'neutral'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"df.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:48.190247Z","iopub.execute_input":"2024-08-13T06:23:48.190577Z","iopub.status.idle":"2024-08-13T06:23:48.200835Z","shell.execute_reply.started":"2024-08-13T06:23:48.190553Z","shell.execute_reply":"2024-08-13T06:23:48.199690Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"id                      object\ntext                    object\nexample_very_unclear      bool\nadmiration               int64\namusement                int64\nanger                    int64\nannoyance                int64\napproval                 int64\ncaring                   int64\nconfusion                int64\ncuriosity                int64\ndesire                   int64\ndisappointment           int64\ndisapproval              int64\ndisgust                  int64\nembarrassment            int64\nexcitement               int64\nfear                     int64\ngratitude                int64\ngrief                    int64\njoy                      int64\nlove                     int64\nnervousness              int64\noptimism                 int64\npride                    int64\nrealization              int64\nrelief                   int64\nremorse                  int64\nsadness                  int64\nsurprise                 int64\nneutral                  int64\ndtype: object"},"metadata":{}}]},{"cell_type":"code","source":"df = df[:-1]","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:48.202373Z","iopub.execute_input":"2024-08-13T06:23:48.202836Z","iopub.status.idle":"2024-08-13T06:23:48.209754Z","shell.execute_reply.started":"2024-08-13T06:23:48.202799Z","shell.execute_reply":"2024-08-13T06:23:48.208653Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# combining 28 features into 6 classes\nl1 = [4, 16, 20, 21, 26] # joy feature\nl2 = [5, 6, 14, 19, 27, 28] # anger feature\nl3 = [3, 11, 24, 25, 29, 30] #desire feature\nl4 = [7, 8, 10, 18, 23] #curiosity feature\nl5 = [12, 13, 15] #disappointment feature\nl6 = [9, 17, 22] # confusion feature","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:48.211628Z","iopub.execute_input":"2024-08-13T06:23:48.212013Z","iopub.status.idle":"2024-08-13T06:23:48.221056Z","shell.execute_reply.started":"2024-08-13T06:23:48.211978Z","shell.execute_reply":"2024-08-13T06:23:48.220236Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"dct = {0 : 'joy', 1 : 'anger', 2 : 'desire', 3:'curiosity', 4 : 'disappointment', 5 : 'confusion'}","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:48.222524Z","iopub.execute_input":"2024-08-13T06:23:48.222862Z","iopub.status.idle":"2024-08-13T06:23:48.233161Z","shell.execute_reply.started":"2024-08-13T06:23:48.222832Z","shell.execute_reply":"2024-08-13T06:23:48.232161Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"dt = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:48.235838Z","iopub.execute_input":"2024-08-13T06:23:48.236100Z","iopub.status.idle":"2024-08-13T06:23:48.249925Z","shell.execute_reply.started":"2024-08-13T06:23:48.236077Z","shell.execute_reply":"2024-08-13T06:23:48.248800Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# creating new data frame\ndt['text'] = df['text'].values\ndt['example_very_unclear'] = df['example_very_unclear']","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:48.251166Z","iopub.execute_input":"2024-08-13T06:23:48.251870Z","iopub.status.idle":"2024-08-13T06:23:48.281685Z","shell.execute_reply.started":"2024-08-13T06:23:48.251834Z","shell.execute_reply":"2024-08-13T06:23:48.280694Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"lt = np.empty(shape=(df.shape[0], 6))\nvals = df.iloc[:,3:].values","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:48.283207Z","iopub.execute_input":"2024-08-13T06:23:48.283608Z","iopub.status.idle":"2024-08-13T06:23:48.309210Z","shell.execute_reply.started":"2024-08-13T06:23:48.283574Z","shell.execute_reply":"2024-08-13T06:23:48.308097Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# combining features \nfor index, col in enumerate(vals):\n    flg = 0\n    for i in l1:\n        flg += col[i-3]\n    if flg >= 1:\n        lt[index][0] = 1\n        continue\n    for i in l2:\n        flg += col[i-3]\n    if flg >= 1:\n        lt[index][1] = 1\n        continue\n    for i in l3:\n        flg += col[i-3]\n    if flg >= 1:\n        lt[index][2] = 1\n        continue\n    for i in l4:\n        flg += col[i-3]\n    if flg >= 1:\n        lt[index][3] = 1\n        continue\n    for i in l5:\n        flg += col[i-3]\n    if flg >= 1:\n        lt[index][4] = 1\n        continue\n    for i in l6:\n        flg += col[i-3]\n    if flg >= 1:\n        lt[index][5] = 1\n        continue","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:48.310823Z","iopub.execute_input":"2024-08-13T06:23:48.311530Z","iopub.status.idle":"2024-08-13T06:23:50.134536Z","shell.execute_reply.started":"2024-08-13T06:23:48.311490Z","shell.execute_reply":"2024-08-13T06:23:50.133232Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"## if there is a new feature that doesnt exits then create a new df and store it there\nnary = np.empty(shape=(df.shape[0]))\n\nfor i in range(6):\n    for j in range(nary.shape[0]):\n        nary[j] = int(lt[j][i])\n#     print(nary)\n    dt[dct[i]] = pd.DataFrame(nary)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:50.142026Z","iopub.execute_input":"2024-08-13T06:23:50.142952Z","iopub.status.idle":"2024-08-13T06:23:51.364191Z","shell.execute_reply.started":"2024-08-13T06:23:50.142909Z","shell.execute_reply":"2024-08-13T06:23:51.363211Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"dct","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:51.365242Z","iopub.execute_input":"2024-08-13T06:23:51.365541Z","iopub.status.idle":"2024-08-13T06:23:51.372729Z","shell.execute_reply.started":"2024-08-13T06:23:51.365516Z","shell.execute_reply":"2024-08-13T06:23:51.371655Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{0: 'joy',\n 1: 'anger',\n 2: 'desire',\n 3: 'curiosity',\n 4: 'disappointment',\n 5: 'confusion'}"},"metadata":{}}]},{"cell_type":"code","source":"dt = dt[dt['example_very_unclear'] == False]","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:51.374037Z","iopub.execute_input":"2024-08-13T06:23:51.374380Z","iopub.status.idle":"2024-08-13T06:23:51.400957Z","shell.execute_reply.started":"2024-08-13T06:23:51.374330Z","shell.execute_reply":"2024-08-13T06:23:51.399970Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"dt.index = range(dt.shape[0])","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:51.402244Z","iopub.execute_input":"2024-08-13T06:23:51.402575Z","iopub.status.idle":"2024-08-13T06:23:51.407384Z","shell.execute_reply.started":"2024-08-13T06:23:51.402548Z","shell.execute_reply":"2024-08-13T06:23:51.406302Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"dt","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:51.408530Z","iopub.execute_input":"2024-08-13T06:23:51.409379Z","iopub.status.idle":"2024-08-13T06:23:51.438936Z","shell.execute_reply.started":"2024-08-13T06:23:51.409344Z","shell.execute_reply":"2024-08-13T06:23:51.437801Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                                     text  \\\n0                                         That game hurt.   \n1          You do right, if you don't care then fuck 'em!   \n2                                      Man I love reddit.   \n3       [NAME] was nowhere near them, he was by the Fa...   \n4       Right? Considering it’s such an important docu...   \n...                                                   ...   \n207809  Well, I'm glad you're out of all that now. How...   \n207810                             Everyone likes [NAME].   \n207811  Well when you’ve imported about a gazillion of...   \n207812                                 That looks amazing   \n207813  The FDA has plenty to criticize. But like here...   \n\n        example_very_unclear  joy  anger  desire  curiosity  disappointment  \\\n0                      False  0.0    1.0     0.0        0.0             0.0   \n1                      False  0.0    0.0     1.0        0.0             0.0   \n2                      False  1.0    0.0     0.0        0.0             0.0   \n3                      False  0.0    0.0     1.0        0.0             0.0   \n4                      False  0.0    0.0     0.0        1.0             0.0   \n...                      ...  ...    ...     ...        ...             ...   \n207809                 False  1.0    0.0     0.0        0.0             0.0   \n207810                 False  1.0    0.0     0.0        0.0             0.0   \n207811                 False  0.0    0.0     0.0        1.0             0.0   \n207812                 False  0.0    0.0     1.0        0.0             0.0   \n207813                 False  0.0    1.0     0.0        0.0             0.0   \n\n        confusion  \n0             0.0  \n1             0.0  \n2             0.0  \n3             0.0  \n4             0.0  \n...           ...  \n207809        0.0  \n207810        0.0  \n207811        0.0  \n207812        0.0  \n207813        0.0  \n\n[207814 rows x 8 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>example_very_unclear</th>\n      <th>joy</th>\n      <th>anger</th>\n      <th>desire</th>\n      <th>curiosity</th>\n      <th>disappointment</th>\n      <th>confusion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>That game hurt.</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>You do right, if you don't care then fuck 'em!</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Man I love reddit.</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Right? Considering it’s such an important docu...</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>207809</th>\n      <td>Well, I'm glad you're out of all that now. How...</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>207810</th>\n      <td>Everyone likes [NAME].</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>207811</th>\n      <td>Well when you’ve imported about a gazillion of...</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>207812</th>\n      <td>That looks amazing</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>207813</th>\n      <td>The FDA has plenty to criticize. But like here...</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>207814 rows × 8 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dct","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:51.440386Z","iopub.execute_input":"2024-08-13T06:23:51.440656Z","iopub.status.idle":"2024-08-13T06:23:51.448762Z","shell.execute_reply.started":"2024-08-13T06:23:51.440633Z","shell.execute_reply":"2024-08-13T06:23:51.447760Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"{0: 'joy',\n 1: 'anger',\n 2: 'desire',\n 3: 'curiosity',\n 4: 'disappointment',\n 5: 'confusion'}"},"metadata":{}}]},{"cell_type":"code","source":"for k in dct.items():\n    print(k)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:51.450236Z","iopub.execute_input":"2024-08-13T06:23:51.450607Z","iopub.status.idle":"2024-08-13T06:23:51.456310Z","shell.execute_reply.started":"2024-08-13T06:23:51.450574Z","shell.execute_reply":"2024-08-13T06:23:51.455445Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"(0, 'joy')\n(1, 'anger')\n(2, 'desire')\n(3, 'curiosity')\n(4, 'disappointment')\n(5, 'confusion')\n","output_type":"stream"}]},{"cell_type":"code","source":"label = []\nfor i in dt.iloc:\n    ps = True\n    for k, v in dct.items():\n        ps = False\n        if i[v] == 1:\n            label.append(k)\n            break\n    if ps:\n        label.append(0)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:23:51.457467Z","iopub.execute_input":"2024-08-13T06:23:51.457725Z","iopub.status.idle":"2024-08-13T06:24:06.753527Z","shell.execute_reply.started":"2024-08-13T06:23:51.457693Z","shell.execute_reply":"2024-08-13T06:24:06.752355Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"dt['label']= label","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:24:06.754827Z","iopub.execute_input":"2024-08-13T06:24:06.755113Z","iopub.status.idle":"2024-08-13T06:24:06.850523Z","shell.execute_reply.started":"2024-08-13T06:24:06.755088Z","shell.execute_reply":"2024-08-13T06:24:06.849275Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"dt.to_csv('processed_emotions_dataset.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:24:06.852051Z","iopub.execute_input":"2024-08-13T06:24:06.852529Z","iopub.status.idle":"2024-08-13T06:24:08.668166Z","shell.execute_reply.started":"2024-08-13T06:24:06.852504Z","shell.execute_reply":"2024-08-13T06:24:08.667182Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:24:08.669515Z","iopub.execute_input":"2024-08-13T06:24:08.669822Z","iopub.status.idle":"2024-08-13T06:24:22.260033Z","shell.execute_reply.started":"2024-08-13T06:24:08.669795Z","shell.execute_reply":"2024-08-13T06:24:22.258839Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import accuracy_score,matthews_corrcoef\n\nfrom tqdm import tqdm, trange,tnrange,tqdm_notebook\nimport random\nimport os\nimport io","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:24:22.261614Z","iopub.execute_input":"2024-08-13T06:24:22.261940Z","iopub.status.idle":"2024-08-13T06:24:27.723388Z","shell.execute_reply.started":"2024-08-13T06:24:22.261912Z","shell.execute_reply":"2024-08-13T06:24:27.722596Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:24:27.724683Z","iopub.execute_input":"2024-08-13T06:24:27.725371Z","iopub.status.idle":"2024-08-13T06:24:27.730031Z","shell.execute_reply.started":"2024-08-13T06:24:27.725324Z","shell.execute_reply":"2024-08-13T06:24:27.728960Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"!pip install torch torchvision torchaudio","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:24:27.731334Z","iopub.execute_input":"2024-08-13T06:24:27.731625Z","iopub.status.idle":"2024-08-13T06:24:40.353247Z","shell.execute_reply.started":"2024-08-13T06:24:27.731601Z","shell.execute_reply":"2024-08-13T06:24:40.352110Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.5.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.32.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2024.7.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())\nprint(torch.cuda.get_device_name(0))","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:24:40.354852Z","iopub.execute_input":"2024-08-13T06:24:40.355164Z","iopub.status.idle":"2024-08-13T06:24:40.410565Z","shell.execute_reply.started":"2024-08-13T06:24:40.355136Z","shell.execute_reply":"2024-08-13T06:24:40.409606Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"True\nTesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\n\nSEED = 19\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == torch.device(\"cuda\"):\n    torch.cuda.manual_seed_all(SEED)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:24:40.411881Z","iopub.execute_input":"2024-08-13T06:24:40.412225Z","iopub.status.idle":"2024-08-13T06:24:40.419143Z","shell.execute_reply.started":"2024-08-13T06:24:40.412194Z","shell.execute_reply":"2024-08-13T06:24:40.418170Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"dt.drop(['example_very_unclear'], axis = 1, inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:24:40.420590Z","iopub.execute_input":"2024-08-13T06:24:40.420937Z","iopub.status.idle":"2024-08-13T06:24:40.460822Z","shell.execute_reply.started":"2024-08-13T06:24:40.420912Z","shell.execute_reply":"2024-08-13T06:24:40.459974Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"dt.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:24:40.462058Z","iopub.execute_input":"2024-08-13T06:24:40.462491Z","iopub.status.idle":"2024-08-13T06:24:40.480648Z","shell.execute_reply.started":"2024-08-13T06:24:40.462460Z","shell.execute_reply":"2024-08-13T06:24:40.479765Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                                                text  joy  anger  desire  \\\n0                                    That game hurt.  0.0    1.0     0.0   \n1     You do right, if you don't care then fuck 'em!  0.0    0.0     1.0   \n2                                 Man I love reddit.  1.0    0.0     0.0   \n3  [NAME] was nowhere near them, he was by the Fa...  0.0    0.0     1.0   \n4  Right? Considering it’s such an important docu...  0.0    0.0     0.0   \n\n   curiosity  disappointment  confusion  label  \n0        0.0             0.0        0.0      1  \n1        0.0             0.0        0.0      2  \n2        0.0             0.0        0.0      0  \n3        0.0             0.0        0.0      2  \n4        1.0             0.0        0.0      3  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>joy</th>\n      <th>anger</th>\n      <th>desire</th>\n      <th>curiosity</th>\n      <th>disappointment</th>\n      <th>confusion</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>That game hurt.</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>You do right, if you don't care then fuck 'em!</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Man I love reddit.</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Right? Considering it’s such an important docu...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## create label and sentence list\nsentences = dt.text.values[1:100000]\n\n#check distribution of data based on labels\nprint(\"Distribution of data based on labels: \",dt.label[1:100000].value_counts())\n\nMAX_LEN = 256\n\n## Import BERT tokenizer, that is used to convert our text into tokens that corresponds to BERT library\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)\ninput_ids = [tokenizer.encode(sent, add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True) for sent in sentences]\nlabels = dt.label.values[1:100000]\n\nprint(\"Actual sentence before tokenization: \",sentences[2])\nprint(\"Encoded Input from dataset: \",input_ids[2])\n\n## Create attention mask\nattention_masks = []\n## Create a mask of 1 for all input tokens and 0 for all padding tokens\nattention_masks = [[float(i>0) for i in seq] for seq in input_ids]\nprint(attention_masks[2])","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:24:40.481759Z","iopub.execute_input":"2024-08-13T06:24:40.482010Z","iopub.status.idle":"2024-08-13T06:25:48.032679Z","shell.execute_reply.started":"2024-08-13T06:24:40.481989Z","shell.execute_reply":"2024-08-13T06:25:48.031663Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Distribution of data based on labels:  label\n2    40962\n3    18090\n1    15545\n0    14480\n4     7109\n5     3813\nName: count, dtype: int64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1b7b33284df448c85cb998aeb9e8493"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b567e11c3a2a4c34abeb628acabeaa96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aef1a2cb3a7646768f393c74c77d2080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8687eb9dfaa347d586224056dde509f0"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2760: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Actual sentence before tokenization:  [NAME] was nowhere near them, he was by the Falcon. \nEncoded Input from dataset:  [101, 1031, 2171, 1033, 2001, 7880, 2379, 2068, 1010, 2002, 2001, 2011, 1996, 11684, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","output_type":"stream"}]},{"cell_type":"code","source":"train_inputs,validation_inputs,train_labels,validation_labels = train_test_split(input_ids,labels,random_state=41,test_size=0.1)\ntrain_masks,validation_masks,_,_ = train_test_split(attention_masks,input_ids,random_state=41,test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:25:48.034344Z","iopub.execute_input":"2024-08-13T06:25:48.035139Z","iopub.status.idle":"2024-08-13T06:25:48.124807Z","shell.execute_reply.started":"2024-08-13T06:25:48.035102Z","shell.execute_reply":"2024-08-13T06:25:48.124007Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# convert all our data into torch tensors, required data type for our model\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\n\n# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 32\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\ntrain_data = TensorDataset(train_inputs,train_masks,train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs,validation_masks,validation_labels)\nvalidation_sampler = RandomSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data,sampler=validation_sampler,batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:25:48.126327Z","iopub.execute_input":"2024-08-13T06:25:48.126737Z","iopub.status.idle":"2024-08-13T06:26:03.971522Z","shell.execute_reply.started":"2024-08-13T06:25:48.126705Z","shell.execute_reply":"2024-08-13T06:26:03.970648Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6).to(device)\n\n# Parameters:\nlr = 2e-5\nadam_epsilon = 1e-8\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 3\n\nnum_warmup_steps = 0\nnum_training_steps = len(train_dataloader)*epochs\n\n### In Transformers, optimizer and schedules are splitted and instantiated like this:\noptimizer = AdamW(model.parameters(), lr=lr,eps=adam_epsilon,correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)  # PyTorch scheduler","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:26:03.972591Z","iopub.execute_input":"2024-08-13T06:26:03.972877Z","iopub.status.idle":"2024-08-13T06:26:06.947965Z","shell.execute_reply.started":"2024-08-13T06:26:03.972854Z","shell.execute_reply":"2024-08-13T06:26:06.946950Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c781da09c0174e72afddb55752ca3d5b"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss_set = []\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}\")\n    model.train()\n    total_loss = 0\n\n    for step, batch in enumerate(train_dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n        loss = outputs[0]\n        total_loss += loss.item()\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        model.zero_grad()\n\n    avg_train_loss = total_loss / len(train_dataloader)\n    train_loss_set.append(avg_train_loss)\n    print(f\"\\tAverage Training Loss: {avg_train_loss}\")\n\n    # Save the model's weights after each epoch\n    model.save_pretrained(f\"./bert_sentiment_epoch_{epoch + 1}\")\n    tokenizer.save_pretrained(f\"./bert_sentiment_epoch_{epoch + 1}\")\n\n    # Validation\n    model.eval()\n    eval_accuracy, eval_mcc_accuracy, nb_eval_steps = 0, 0, 0\n\n    for batch in validation_dataloader:\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n\n        with torch.no_grad():\n            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n        logits = logits[0].detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        pred_flat = np.argmax(logits, axis=1).flatten()\n        labels_flat = label_ids.flatten()\n\n        tmp_eval_accuracy = accuracy_score(labels_flat, pred_flat)\n        tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n\n        eval_accuracy += tmp_eval_accuracy\n        eval_mcc_accuracy += tmp_eval_mcc_accuracy\n        nb_eval_steps += 1\n\n    print(f\"\\tValidation Accuracy: {eval_accuracy / nb_eval_steps}\")\n    print(f\"\\tValidation MCC Accuracy: {eval_mcc_accuracy / nb_eval_steps}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-13T06:27:00.868292Z","iopub.execute_input":"2024-08-13T06:27:00.868652Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1\n\tAverage Training Loss: 1.1626203925084948\n\tValidation Accuracy: 0.5767771565495208\n\tValidation MCC Accuracy: 0.40876682104542883\nEpoch 2\n\tAverage Training Loss: 1.030245387740632\n\tValidation Accuracy: 0.5725838658146964\n\tValidation MCC Accuracy: 0.41935118093014173\nEpoch 3\n","output_type":"stream"}]},{"cell_type":"code","source":"def test_sentiment(sentences):\n    model.eval()\n    inputs = [tokenizer.encode(sent, add_special_tokens=True, max_length=MAX_LEN, pad_to_max_length=True) for sent in sentences]\n    masks = [[float(i > 0) for i in seq] for seq in inputs]\n\n    inputs = torch.tensor(inputs).to(device)\n    masks = torch.tensor(masks).to(device)\n\n    with torch.no_grad():\n        outputs = model(inputs, token_type_ids=None, attention_mask=masks)\n        logits = outputs[0].detach().cpu().numpy()\n\n    predictions = np.argmax(logits, axis=1).flatten()\n    return [dct[pred] for pred in predictions]\n\ntest_sentences = [\"I love this!\", \"This is so frustrating.\",\"i am curious about this topic\"]\nprint(test_sentiment(test_sentences))","metadata":{"execution":{"iopub.status.busy":"2024-08-13T08:58:13.698790Z","iopub.execute_input":"2024-08-13T08:58:13.699165Z","iopub.status.idle":"2024-08-13T08:58:13.742341Z","shell.execute_reply.started":"2024-08-13T08:58:13.699136Z","shell.execute_reply":"2024-08-13T08:58:13.741473Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"['joy', 'anger', 'curiosity']\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model.state_dict(), 'SavedWeights.pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}